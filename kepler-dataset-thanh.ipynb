{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2c4b45",
   "metadata": {},
   "source": [
    "# Cấu hình chung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4db6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_DATABASE = flags.DEFINE_string(\"database\", None, \"Database name.\")\n",
    "flags.mark_flag_as_required(\"database\")\n",
    "_USER = flags.DEFINE_string(\"user\", None, \"Database username.\")\n",
    "_PASSWORD = flags.DEFINE_string(\"password\", None, \"Database password.\")\n",
    "_HOST = flags.DEFINE_string(\"host\", \"localhost\", \"Database host.\")\n",
    "_SEED = flags.DEFINE_float(\"seed\", 0, \"Database random number seed.\")\n",
    "\n",
    "_TEMPLATE_FILE = flags.DEFINE_string(\"template_file\", None,\n",
    "                                     \"Parameterized query template file.\")\n",
    "flags.mark_flag_as_required(\"template_file\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517a8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATABASE = 'kepler_stack'\n",
    "_USER = 'kepler_user'\n",
    "_PASSWORD = '12345'\n",
    "_HOST = 'localhost'\n",
    "_SEED = 0\n",
    "_TEMPLATE_FILE = 'inputs/stack_query_templates.json'\n",
    "\n",
    "_ROOT_OUTPUT_DIR = 'outputs'\n",
    "\n",
    "_QUERY = 'q1_0'\n",
    "\n",
    "_MAX_WORKERS = 1 # Giảm số worker để tránh quá tải CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abc55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(_ROOT_OUTPUT_DIR, exist_ok= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83daa264",
   "metadata": {},
   "source": [
    "# I. Gen các giá trị parameter cho query templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_PARAMETER_COUNT = flags.DEFINE_integer(\n",
    "    \"count\", 1000000, \"The max number of parameters to generate per query.\")\n",
    "\n",
    "_COUNTS_OUTPUT_FILE = flags.DEFINE_string(\n",
    "    \"counts_output_file\", None,\n",
    "    \"Output file to store the parameter counts per query.\")\n",
    "flags.mark_flag_as_required(\"counts_output_file\")\n",
    "_PARAMETERS_OUTPUT_DIR = flags.DEFINE_string(\n",
    "    \"parameters_output_dir\", None,\n",
    "    \"Directory to store parameter values per query.\")\n",
    "flags.mark_flag_as_required(\"parameters_output_dir\")\n",
    "\n",
    "_DRY_RUN = flags.DEFINE_bool(\n",
    "    \"dry_run\", False,\n",
    "    \"If true, verify that the parameter generation process works correctly \"\n",
    "    \"using a single, non-random parameter value. This involves a) verifying \"\n",
    "    \"that the parameter generation query can be composed from the template \"\n",
    "    \"query and b) ensuring that the template query executes successfully with \"\n",
    "    \"the generated parameter value.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27678427",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PARAMETER_COUNT = 1\n",
    "\n",
    "_COUNTS_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}/parameter_counts'\n",
    "_COUNTS_OUTPUT_FILE = f'{_COUNTS_OUTPUT_DIR}/parameter_counts.json'\n",
    "\n",
    "_PARAMETERS_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}/parameters/'\n",
    "\n",
    "_DRY_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f28d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(_COUNTS_OUTPUT_DIR, exist_ok= True)\n",
    "os.makedirs(_PARAMETERS_OUTPUT_DIR, exist_ok= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69402122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT site.site_name,tag.name\n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "AND site.site_name IS NOT NULL\n",
      "AND tag.name IS NOT NULL\n",
      "GROUP BY site.site_name,tag.name \n",
      "ORDER BY random()\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from concurrent import futures\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import parameter_generator\n",
    "import query_utils\n",
    "\n",
    "\n",
    "with open(_TEMPLATE_FILE) as f:\n",
    "  templates = json.load(f)\n",
    "\n",
    "work_list = []\n",
    "# Query templates that failed hint verification using the parameters from the\n",
    "# original Stack benchmark. That is, at least one provided hint was ignored by\n",
    "# the PG optimizer for at least one parameter binding.\n",
    "skip_list = [\"q3_0\", \"q3_1\", \"q3_2\"]\n",
    "for query_id, template in templates.items():\n",
    "  if query_id not in skip_list and query_id == _QUERY:\n",
    "    work_list.append(\n",
    "        parameter_generator.TemplateItem(\n",
    "            query_id=query_id, template=template))\n",
    "\n",
    "database_configuration = query_utils.DatabaseConfiguration(\n",
    "    dbname=_DATABASE,\n",
    "    user=_USER,\n",
    "    password=_PASSWORD,\n",
    "    host=_HOST,\n",
    "    seed=_SEED)\n",
    "generator = parameter_generator.ParameterGenerator(database_configuration)\n",
    "parameter_generation_function = functools.partial(\n",
    "    generator.generate_parameters, _PARAMETER_COUNT, dry_run=_DRY_RUN)\n",
    "\n",
    "output_counts = collections.defaultdict(lambda: {})\n",
    "# The high-latency work occurs remotely via the database executing queries to\n",
    "# generate parameters. The number of max workers is limited empirically to\n",
    "# avoid memory issues on the database side.\n",
    "with futures.ThreadPoolExecutor(max_workers=_MAX_WORKERS) as executor:\n",
    "  for result in executor.map(parameter_generation_function, work_list):\n",
    "    query_id = next(iter(result))\n",
    "    logging.info(\"Finished generating for %s\", query_id)\n",
    "    with open(\n",
    "        os.path.join(_PARAMETERS_OUTPUT_DIR,\n",
    "                      f\"{query_id}-{len(result[query_id]['params'])}.json\"),\n",
    "        \"w\") as f:\n",
    "      json.dump(result, f)\n",
    "\n",
    "    output_counts[query_id] = len(result[query_id][\"params\"])\n",
    "\n",
    "    if _DRY_RUN:\n",
    "      # Ensure that the template query executes successfully with the\n",
    "      # generated parameter value.\n",
    "      query_manager = query_utils.QueryManager(database_configuration)\n",
    "      start_ms = int(time.time() * 1e3)\n",
    "      query = result[query_id][\"query\"]\n",
    "      params = result[query_id][\"params\"][0]\n",
    "      results = query_manager.execute(query, params)\n",
    "      end_ms = int(time.time() * 1e3)\n",
    "\n",
    "      print(f\"Query {query_id} approximate latency: {end_ms-start_ms} ms\")\n",
    "      print(f\"Template: {query}\")\n",
    "      print(f\"Params: {params}\")\n",
    "\n",
    "      # The query should return at least one result.\n",
    "      assert results\n",
    "\n",
    "with open(_COUNTS_OUTPUT_FILE, \"w\") as f:\n",
    "  json.dump(output_counts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde840d",
   "metadata": {},
   "source": [
    "# II. Gen Plan Candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6b0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import json\n",
    "import os\n",
    "\n",
    "import main_utils\n",
    "import pg_generate_plan_candidates\n",
    "import pg_plan_hint_extractor\n",
    "import query_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689001da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationFunction(enum.Enum):\n",
    "  PG_CONFIGS = \"pg_configs\"\n",
    "  ROW_NUM_EVOLUTION = \"row_num_evolution\"\n",
    "  EXHAUSTIVE_CARDINALITY_PERTURBATIONS = \"exhaustive_cardinality_perturbations\"\n",
    "\n",
    "\n",
    "_GENERATION_FUNCTION_MAP = {\n",
    "    GenerationFunction.PG_CONFIGS:\n",
    "        pg_generate_plan_candidates.get_query_plans,\n",
    "    GenerationFunction.ROW_NUM_EVOLUTION:\n",
    "        pg_generate_plan_candidates.generate_by_row_num_evolution,\n",
    "    GenerationFunction.EXHAUSTIVE_CARDINALITY_PERTURBATIONS:\n",
    "        pg_generate_plan_candidates\n",
    "        .generate_by_exhaustive_cardinality_perturbations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf8f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _supports_distributed_execution(\n",
    "    generation_function: GenerationFunction) -> bool:\n",
    "  return generation_function != GenerationFunction.EXHAUSTIVE_CARDINALITY_PERTURBATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_QUERY_PARAMS_FILE = flags.DEFINE_string(\n",
    "    \"query_params_file\", None,\n",
    "    \"File containing parameterized queries with list of parameter values.\")\n",
    "flags.mark_flag_as_required(\"query_params_file\")\n",
    "_PARAMS_LIMIT = flags.DEFINE_integer(\n",
    "    \"params_limit\", None,\n",
    "    \"The number of parameter values to use when generating plans.\")\n",
    "_OUTPUT_DIR = flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"Directory in which to store query plan hints and configs.\")\n",
    "flags.mark_flag_as_required(\"output_dir\")\n",
    "\n",
    "_PLANS_OUTPUT_FILE = flags.DEFINE_string(\n",
    "    \"plans_output_file\", None,\n",
    "    \"File to store distinct plans per query. The file name is expected to end in .json\"\n",
    ")\n",
    "flags.mark_flag_as_required(\"plans_output_file\")\n",
    "_PLAN_INDEX_SUFFIX = flags.DEFINE_string(\n",
    "    \"plan_index_suffix\", \"_plan_index.json\",\n",
    "    \"Suffix of files to store plan indices in.\")\n",
    "_VERIFICATION_FAILURES_FILE = flags.DEFINE_string(\n",
    "    \"verification_failures_file\", \"verification_failures.json\",\n",
    "    \"Filename of file to save verification failures.\")\n",
    "_CHUNKSIZE = flags.DEFINE_integer(\n",
    "    \"chunksize\", 100, \"How many params to include in each subprocess chunk.\")\n",
    "_KEYS_TO_REMOVE = flags.DEFINE_list(\n",
    "    \"keys_to_remove\", [],\n",
    "    (\"List of keys to filter from EXPLAIN plan JSON. Good candidates include \"\n",
    "     \"\\\"Parallel Aware\\\", \\\"Relation Name\\\", \\\"Parent Relationship\\\"\"))\n",
    "\n",
    "_GENERATION_FUNCTION = flags.DEFINE_enum_class(\n",
    "    \"generation_function\", GenerationFunction.PG_CONFIGS.value,\n",
    "    GenerationFunction, \"Which plan generation function to use.\")\n",
    "_SOFT_TOTAL_PLANS_LIMIT = flags.DEFINE_integer(\n",
    "    \"soft_total_plans_limit\", None,\n",
    "    \"Soft limit on total number of plans to produce.\"\n",
    ")\n",
    "# Pg configs flags.\n",
    "_CONFIG_STR = flags.DEFINE_string(\n",
    "    \"configs\", \"\",\n",
    "    \"Comma-separated string of Postgres optimizer configuration parameters to toggle off.\"\n",
    ")\n",
    "# Row number evolution flags.\n",
    "_MAX_PLANS_PER_PARAM = flags.DEFINE_integer(\n",
    "    \"max_plans_per_param\", None,\n",
    "    \"Stop evolution after this number of plans is exceeded.\")\n",
    "_NUM_GENERATIONS = flags.DEFINE_integer(\n",
    "    \"num_generations\", 3, \"Number of generations of row number evolution.\")\n",
    "_NUM_MUTATIONS_PER_PLAN = flags.DEFINE_integer(\n",
    "    \"num_mutations_per_plan\", 25, \"Number of random mutations for each plan.\")\n",
    "_EXPONENT_BASE = flags.DEFINE_integer(\n",
    "    \"exponent_base\", 10, \"Base of exponential row number perturbations.\")\n",
    "_EXPONENT_RANGE = flags.DEFINE_integer(\n",
    "    \"exponent_range\", 3, \"One-sided range of exponent of perturbations.\")\n",
    "_MAX_PLANS_PER_GENERATION = flags.DEFINE_integer(\n",
    "    \"max_plans_per_generation\", 20,\n",
    "    \"Max number of plans to mutate per generation.\")\n",
    "_PERTURB_UNIT_ONLY = flags.DEFINE_bool(\n",
    "    \"perturb_unit_only\", True,\n",
    "    \"Whether to perturb only row counts exactly equal to one.\"\n",
    ")\n",
    "_MAX_PERTURBS_PER_JOIN = flags.DEFINE_integer(\n",
    "    \"max_perturbs_per_join\", 1,\n",
    "    \"Limit on how many times a specific join can be perturbed.\"\n",
    ")\n",
    "# Exhaustive cardinality perturbation flags.\n",
    "_CARDINALITY_MULTIPLIERS = flags.DEFINE_list(\n",
    "    \"cardinality_multipliers\", None,\n",
    "    \"List of cardinality multipliers to apply when generating plans.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc979618",
   "metadata": {},
   "outputs": [],
   "source": [
    "_QUERY_PARAMS_FILE = f'{_PARAMETERS_OUTPUT_DIR}/q1_0-1.json' # Thêm đường dẫn tới file json kết quả của phần I.\n",
    "_PARAMS_LIMIT = None\n",
    "_PLANS_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}/plans'\n",
    "\n",
    "_PLANS_OUTPUT_FILE =  'pg_hints.json'\n",
    "\n",
    "_PLAN_INDEX_SUFFIX = '_plan_index.json'\n",
    "\n",
    "_VERIFICATION_FAILURES_FILE = 'verification_failures.json'\n",
    "\n",
    "_CHUNKSIZE = 100\n",
    "_KEYS_TO_REMOVE = []\n",
    "\n",
    "_GENERATION_FUNCTION = GenerationFunction.ROW_NUM_EVOLUTION\n",
    "\n",
    "_SOFT_TOTAL_PLANS_LIMIT = None\n",
    "# Pg configs flags.\n",
    "_CONFIG_STR = None\n",
    "\n",
    "# Row number evolution flags.\n",
    "_MAX_PLANS_PER_PARAM = 10\n",
    "\n",
    "_NUM_GENERATIONS = 3\n",
    "\n",
    "_NUM_MUTATIONS_PER_PLAN = 25\n",
    "\n",
    "_EXPONENT_BASE = 10\n",
    "_EXPONENT_RANGE = 3\n",
    "\n",
    "_MAX_PLANS_PER_GENERATION = 20\n",
    "\n",
    "_PERTURB_UNIT_ONLY = True\n",
    "\n",
    "_MAX_PERTURBS_PER_JOIN = 1\n",
    "\n",
    "# Exhaustive cardinality perturbation flags.\n",
    "_CARDINALITY_MULTIPLIERS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e356228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW seq_page_cost;\n",
      "SHOW random_page_cost;\n",
      "SHOW cpu_tuple_cost;\n",
      "SHOW cpu_index_tuple_cost;\n",
      "SHOW cpu_operator_cost;\n",
      "SHOW parallel_setup_cost;\n",
      "SHOW parallel_tuple_cost;\n",
      "SHOW min_parallel_table_scan_size;\n",
      "SHOW min_parallel_index_scan_size;\n",
      "SHOW effective_cache_size;\n",
      "SHOW jit_above_cost;\n",
      "SHOW jit_inline_above_cost;\n",
      "SHOW jit_optimize_above_cost;\n",
      "SHOW shared_buffers;\n",
      "SHOW huge_pages;\n",
      "SHOW temp_buffers;\n",
      "SHOW max_prepared_transactions;\n",
      "SHOW work_mem;\n",
      "SHOW hash_mem_multiplier;\n",
      "SHOW maintenance_work_mem;\n",
      "SHOW autovacuum_work_mem;\n",
      "SHOW max_stack_depth;\n",
      "SHOW shared_memory_type;\n",
      "SHOW dynamic_shared_memory_type;\n",
      "SHOW temp_file_limit;\n",
      "SHOW max_files_per_process;\n",
      "Start: {query_id}\n",
      "EXPLAIN (FORMAT JSON) select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(site tag #1000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #100000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10) Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #10) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) Rows(site tag #10) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #1000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #100) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #100000) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000000) Rows(site tag #1000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #100) Rows(site tag #100000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(site tag #100) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "Checking 3 hints for query_id q1_0\n",
      "EXPLAIN (FORMAT JSON) /*+  SeqScan(site) SeqScan(tag) IndexOnlyScan(tag_question tag_question_site_id_tag_id_question_id_idx) IndexOnlyScan(question question_pkey) NestLoop(site tag) NestLoop(site tag tag_question) NestLoop(site tag tag_question question) Leading((((site tag) tag_question) question)) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+  SeqScan(tag) IndexOnlyScan(tag_question tag_question_site_id_tag_id_question_id_idx) IndexOnlyScan(question question_pkey) SeqScan(site) NestLoop(tag tag_question) NestLoop(tag tag_question question) HashJoin(tag tag_question question site) Leading((((tag tag_question) question) site)) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+  SeqScan(tag) SeqScan(site) IndexOnlyScan(tag_question tag_question_site_id_tag_id_question_id_idx) IndexOnlyScan(question question_pkey) MergeJoin(tag site) NestLoop(tag site tag_question) NestLoop(tag site tag_question question) Leading((((tag site) tag_question) question)) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "Printing positive failure counts:\n",
      "Query q1_0 failure ratio: 0/3\n",
      "Printing hints counts by source:\n",
      "q1_0: number of suggestions for hint 0 from source default: 1\n",
      "q1_0: number of suggestions for hint 1 from source /*+ Rows(site tag #1000) */: 1\n",
      "q1_0: number of suggestions for hint 2 from source /*+ Rows(question site tag tag_question #1000) Rows(site tag #10) */: 1\n"
     ]
    }
   ],
   "source": [
    "configs = _CONFIG_STR.split(\",\") if _CONFIG_STR else []\n",
    "\n",
    "with open(_QUERY_PARAMS_FILE) as json_file:\n",
    "    info = json.load(json_file)\n",
    "\n",
    "hints_output_dir = os.path.join(_PLANS_OUTPUT_DIR, _DATABASE)\n",
    "os.makedirs(hints_output_dir, exist_ok=True)\n",
    "\n",
    "database_configuration = query_utils.DatabaseConfiguration(\n",
    "    dbname=_DATABASE,\n",
    "    user=_USER,\n",
    "    password=_PASSWORD,\n",
    "    host=_HOST)\n",
    "query_manager = query_utils.QueryManager(database_configuration)\n",
    "query_utils.save_postgres_config_info(query_manager, _PLANS_OUTPUT_DIR)\n",
    "\n",
    "hint_accumulator = main_utils.HintAccumulator()\n",
    "for query_id, query_metadata in info.items():\n",
    "    print(\"Start: {query_id}\")\n",
    "\n",
    "    output = {}\n",
    "    output[\"output\"] = {}\n",
    "\n",
    "    function_kwargs = {\n",
    "        \"database_configuration\": database_configuration,\n",
    "        \"query\": query_metadata[\"query\"],\n",
    "        \"keys_to_remove\": _KEYS_TO_REMOVE\n",
    "    }\n",
    "\n",
    "    # Augment kwargs depending on generation function.\n",
    "    if _GENERATION_FUNCTION == GenerationFunction.PG_CONFIGS:\n",
    "        function_kwargs[\"configs\"] = configs\n",
    "    elif _GENERATION_FUNCTION == GenerationFunction.ROW_NUM_EVOLUTION:\n",
    "        function_kwargs.update({\n",
    "            \"max_plans\": _MAX_PLANS_PER_PARAM,\n",
    "            \"num_generations\": _NUM_GENERATIONS,\n",
    "            \"num_mutations_per_plan\": _NUM_MUTATIONS_PER_PLAN,\n",
    "            \"exponent_base\": _EXPONENT_BASE,\n",
    "            \"exponent_range\": _EXPONENT_RANGE,\n",
    "            \"max_plans_per_generation\": _MAX_PLANS_PER_GENERATION,\n",
    "            \"perturb_unit_only\": _PERTURB_UNIT_ONLY,\n",
    "            \"max_perturbs_per_join\": _MAX_PERTURBS_PER_JOIN\n",
    "        })\n",
    "    elif _GENERATION_FUNCTION == GenerationFunction.EXHAUSTIVE_CARDINALITY_PERTURBATIONS:\n",
    "        cardinality_multipliers = [\n",
    "            float(multiplier) for multiplier in _CARDINALITY_MULTIPLIERS\n",
    "        ]\n",
    "\n",
    "        function_kwargs.update(\n",
    "            {\"cardinality_multipliers\": cardinality_multipliers})\n",
    "\n",
    "    if _PARAMS_LIMIT:\n",
    "        query_metadata[\"params\"] = query_metadata[\"params\"][:_PARAMS_LIMIT]\n",
    "\n",
    "    plan_hint_extractor = pg_plan_hint_extractor.PlanHintExtractor()\n",
    "    pg_generate_plan_candidates.execute_plan_generation(\n",
    "        _GENERATION_FUNCTION_MAP[_GENERATION_FUNCTION],\n",
    "        function_kwargs,\n",
    "        query_metadata[\"params\"],\n",
    "        plan_hint_extractor=plan_hint_extractor,\n",
    "        chunksize=_CHUNKSIZE,\n",
    "        distributed=_supports_distributed_execution(_GENERATION_FUNCTION),\n",
    "        soft_total_plans_limit=_SOFT_TOTAL_PLANS_LIMIT)\n",
    "    counts, plan_hints, params_plan_indices, debug_infos = (\n",
    "        plan_hint_extractor.get_consolidated_plan_hints())\n",
    "\n",
    "    hint_accumulator.query_id_to_counts[query_id] = counts\n",
    "    hint_accumulator.query_id_to_plan_hints[query_id] = plan_hints\n",
    "    hint_accumulator.query_id_to_params_plan_indices[\n",
    "        query_id] = params_plan_indices\n",
    "    hint_accumulator.query_id_to_debug_infos[query_id] = debug_infos\n",
    "\n",
    "    failure_counts = pg_plan_hint_extractor.verify_hints(\n",
    "        query_id=query_id,\n",
    "        query=query_metadata[\"query\"],\n",
    "        plan_hints=plan_hints,\n",
    "        params_plan_indices=params_plan_indices,\n",
    "        database_configuration=database_configuration)\n",
    "    hint_accumulator.combined_failure_counts.update(failure_counts)\n",
    "\n",
    "main_utils.print_failure_counts(hint_accumulator.combined_failure_counts)\n",
    "main_utils.print_hint_counts_by_source(hint_accumulator.query_id_to_counts)\n",
    "\n",
    "hint_accumulator.save(\n",
    "    output_dir=hints_output_dir,\n",
    "    plans_output_file=_PLANS_OUTPUT_FILE,\n",
    "    verification_failures_file=_VERIFICATION_FAILURES_FILE,\n",
    "    plan_index_suffix=_PLAN_INDEX_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64fbf0",
   "metadata": {},
   "source": [
    "# III. Thực thi queries (Chọn 1. hoặc 2. để chạy thôi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PLAN_HINTS_FILE = f'{_PLANS_OUTPUT_DIR}/{_PLANS_OUTPUT_FILE}'\n",
    "_EXECUTION_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82feaa",
   "metadata": {},
   "source": [
    "# 1. Latency + Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import main_utils\n",
    "import pg_execute_training_data_queries\n",
    "import query_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321499b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_EXECUTION_METHOD = flags.DEFINE_string(\n",
    "    \"execution_method\",\n",
    "    \"regular\",\n",
    "    (\n",
    "        \"Which execution method to use: regular to simply time the latency,\"\n",
    "        \" explain, or explain_analyze.\"\n",
    "    ),\n",
    ")\n",
    "_ITERATIONS = flags.DEFINE_integer(\n",
    "    \"iterations\",\n",
    "    3,\n",
    "    (\n",
    "        \"The number of iterations to execute query (query plan, parameter\"\n",
    "        \" binding) pairing.\"\n",
    "    ),\n",
    ")\n",
    "_BATCH_SIZE = flags.DEFINE_integer(\n",
    "    \"batch_size\",\n",
    "    10,\n",
    "    (\n",
    "        \"Batch of parameters for which to collect training data before\"\n",
    "        \" checkpointing.\"\n",
    "    ),\n",
    ")\n",
    "_LIMIT = flags.DEFINE_integer(\n",
    "    \"limit\",\n",
    "    None,\n",
    "    \"Limit the number of parameters per query to gather execution data for.\",\n",
    ")\n",
    "\n",
    "_QUERY_TIMEOUT_MULTIPLIER = flags.DEFINE_integer(\n",
    "    \"query_timeout_multiplier\",\n",
    "    5,\n",
    "    (\n",
    "        \"This factor is multiplied by the median execution time of the default\"\n",
    "        \" query plan to provide an upper bound on the query execution time\"\n",
    "        \" considered 'way too slow' during execution data collection for each\"\n",
    "        \" set of parameter values. This input has an inverse multiplicative\"\n",
    "        \" relationship with query_timeout_minimum_speedup_multiplier. The\"\n",
    "        \" product will be clippedto [query_timeout_min_ms,\"\n",
    "        \" query_timeout_max_ms].\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_QUERY_TIMEOUT_MIN_MS = flags.DEFINE_integer(\n",
    "    \"query_timeout_min_ms\",\n",
    "    200,\n",
    "    (\n",
    "        \"The minimum timeout for each query execution to enable setting a low\"\n",
    "        \" multiplier while balancing the risk of timeouts caused by system\"\n",
    "        \" noise for very fast queries.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_QUERY_TIMEOUT_MAX_MS = flags.DEFINE_integer(\n",
    "    \"query_timeout_max_ms\",\n",
    "    60 * 1000,\n",
    "    (\n",
    "        \"The maximum timeout for each query execution to provide a hard-cap on\"\n",
    "        \" the cost of very slow query plans.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_QUERY_TIMEOUT_MINIMUM_SPEEDUP_MULTIPLIER = flags.DEFINE_integer(\n",
    "    \"query_timeout_minimum_speedup_multiplier\",\n",
    "    1,\n",
    "    (\n",
    "        \"This factor describes the minimum speed up expected from a candidate\"\n",
    "        \" plan to be considered an alternative to the default. Plans that do\"\n",
    "        \" not provide this speed up are considered timed out. This input has an\"\n",
    "        \" inverse multiplicative relationship with query_timeout_multiplier.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_VERIFICATION_FILE = flags.DEFINE_string(\n",
    "    \"verification_file\",\n",
    "    None,\n",
    "    (\n",
    "        \"File containing verification results. If specified, we will only\"\n",
    "        \" execute hints for which there are no failures.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_NUM_INITIAL_DEFAULT_EXECUTIONS = flags.DEFINE_integer(\n",
    "    \"num_initial_default_executions\",\n",
    "    None,\n",
    "    (\n",
    "        \"How many parameters to initially execute default plans for to\"\n",
    "        \" determine the tail latency parameters.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_SLOWEST_DEFAULT_TOP_K = flags.DEFINE_integer(\n",
    "    \"slowest_default_top_k\",\n",
    "    None,\n",
    "    \"Specifies how many of the slowest parameters to sample from.\",\n",
    ")\n",
    "\n",
    "_SLOWEST_DEFAULT_SAMPLE_SIZE = flags.DEFINE_integer(\n",
    "    \"slowest_default_sample_size\",\n",
    "    None,\n",
    "    \"How many of the slowest k parameters to sample.\",\n",
    ")\n",
    "\n",
    "_PLAN_COVER_NUM_PARAMS = flags.DEFINE_integer(\n",
    "    \"plan_cover_num_params\",\n",
    "    None,\n",
    "    (\n",
    "        \"Use the first N parameters to compute the plan cover. If not\"\n",
    "        \" specified, plan cover pruning won't be used.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_NEAR_OPTIMAL_THRESHOLD = flags.DEFINE_float(\n",
    "    \"near_optimal_threshold\",\n",
    "    None,\n",
    "    (\n",
    "        \"Defines what constitutes a near-optimal plan: if latency < \"\n",
    "        \"this value * optimal latency.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "_NUM_PARAMS_THRESHOLD = flags.DEFINE_float(\n",
    "    \"num_params_threshold\",\n",
    "    None,\n",
    "    \"Requires that this proportion of parameters be covered by the plan cover.\",\n",
    ")\n",
    "\n",
    "_RESULTS_FILE = flags.DEFINE_string(\n",
    "    \"results_file\",\n",
    "    None,\n",
    "    \"File containing previous results. Used to resume execution.\",\n",
    ")\n",
    "\n",
    "_METADATA_FILE = flags.DEFINE_string(\n",
    "    \"metadata_file\",\n",
    "    None,\n",
    "    \"File containing previous metadata. Used to resume execution.\",\n",
    ")\n",
    "\n",
    "_QUERY = flags.DEFINE_string(\"query\", None, \"Specific query id to execute.\")\n",
    "\n",
    "_OUTPUT_DIR = flags.DEFINE_string(\n",
    "    \"output_dir\", None, \"Directory to store execution results.\"\n",
    ")\n",
    "flags.mark_flag_as_required(\"output_dir\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_EXECUTION_METHOD = 'explain_analyze' # Kết quả gồm Latency + Explain\n",
    "\n",
    "_ITERATIONS = 3\n",
    "_BATCH_SIZE = 10\n",
    "\n",
    "_LIMIT = None\n",
    "\n",
    "_QUERY_TIMEOUT_MULTIPLIER = 5\n",
    "\n",
    "_QUERY_TIMEOUT_MIN_MS = 200\n",
    "\n",
    "_QUERY_TIMEOUT_MAX_MS = 120 * 1000 # 2 phút\n",
    "\n",
    "_QUERY_TIMEOUT_MINIMUM_SPEEDUP_MULTIPLIER = 1\n",
    "\n",
    "_VERIFICATION_FILE = f'{_PLANS_OUTPUT_DIR}/{_DATABASE}/verification/verification_failures.json'\n",
    "\n",
    "_NUM_INITIAL_DEFAULT_EXECUTIONS = None\n",
    "\n",
    "_SLOWEST_DEFAULT_TOP_K = None\n",
    "\n",
    "_SLOWEST_DEFAULT_SAMPLE_SIZE = None\n",
    "\n",
    "_PLAN_COVER_NUM_PARAMS = None\n",
    "\n",
    "_NEAR_OPTIMAL_THRESHOLD = None\n",
    "\n",
    "_NUM_PARAMS_THRESHOLD = None\n",
    "\n",
    "_RESULTS_FILE = None # Truyền path của file kết quả đã chạy được vào nếu muốn resume   \n",
    "\n",
    "_METADATA_FILE = None # Truyền path của file metadata đã chạy được vào nếu muốn resume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_manager = query_utils.QueryManager(\n",
    "    query_utils.DatabaseConfiguration(\n",
    "        dbname=_DATABASE, user=_USER, password=_PASSWORD\n",
    "    )\n",
    ")\n",
    "query_utils.save_postgres_config_info(query_manager, _EXECUTION_OUTPUT_DIR)\n",
    "\n",
    "with open(_PLAN_HINTS_FILE) as f:\n",
    "    plan_hints = json.load(f)\n",
    "\n",
    "query_id_to_skip_indices = main_utils.get_skip_indices(\n",
    "    plan_hints, _VERIFICATION_FILE\n",
    ")\n",
    "\n",
    "with open(_PARAMETER_VALUES_FILE) as f:\n",
    "    parameter_values = json.load(f)\n",
    "\n",
    "with open(_QUERY_TEMPLATES_FILE) as f:\n",
    "    templates = json.load(f)\n",
    "\n",
    "query_ids = [_QUERY] if _QUERY else plan_hints.keys()\n",
    "\n",
    "output_subdir = os.path.join(_EXECUTION_OUTPUT_DIR, \"execution_output\")\n",
    "os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "def checkpoint_results(query_id: str, results: Any,\n",
    "                        is_metadata: bool) -> None:\n",
    "    type_token = \"_metadata\" if is_metadata else \"\"\n",
    "    with open(\n",
    "        os.path.join(output_subdir,\n",
    "                    f\"{_DATABASE}_{query_id}{type_token}.json\"),\n",
    "        \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "def execute_query(\n",
    "    unused_query_manager: query_utils.QueryManager,\n",
    "    query: str,\n",
    "    params: List[Any],\n",
    "    timeout_ms: Optional[int] = None,\n",
    ") -> Tuple[Optional[float], Optional[int]]:\n",
    "    del unused_query_manager\n",
    "    return query_manager.execute_timed(query, params, timeout_ms)\n",
    "\n",
    "def execute_explain(\n",
    "    unused_query_manager: query_utils.QueryManager,\n",
    "    query: str,\n",
    "    params: List[Any],\n",
    "    _: Optional[int] = None,\n",
    ") -> Tuple[Any, None]:\n",
    "    del unused_query_manager\n",
    "    return query_manager.get_query_plan(query, params), None\n",
    "\n",
    "def execute_explain_analyze(\n",
    "    unused_query_manager: query_utils.QueryManager,\n",
    "    query: str,\n",
    "    params: List[Any],\n",
    "    _: Optional[int] = None,\n",
    ") -> Tuple[Any, None]:\n",
    "    del unused_query_manager\n",
    "    return query_manager.get_query_plan_and_execute(query, params), None\n",
    "\n",
    "execution_method_map = {\n",
    "    \"regular\": (execute_query, \"duration_ms\"),\n",
    "    \"explain\": (execute_explain, \"explain_output\"),\n",
    "    \"explain_analyze\": (execute_explain_analyze, \"explain_analyze_output\")\n",
    "}\n",
    "execution_method, results_key = execution_method_map[_EXECUTION_METHOD]\n",
    "\n",
    "previous_results = None\n",
    "previous_metadata = None\n",
    "if _RESULTS_FILE:\n",
    "    with open(_RESULTS_FILE) as f:\n",
    "    previous_results = json.load(f)\n",
    "\n",
    "if _METADATA_FILE:\n",
    "    with open(_METADATA_FILE) as f:\n",
    "    previous_metadata = json.load(f)\n",
    "\n",
    "for query_id in query_ids:\n",
    "    pg_execute_training_data_queries.execute_training_data_queries(\n",
    "        batch_index=0,\n",
    "        parameter_values=parameter_values,\n",
    "        query_id=query_id,\n",
    "        templates=templates,\n",
    "        plan_hints=plan_hints,\n",
    "        iterations=_ITERATIONS,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        skip_indices=query_id_to_skip_indices.get(query_id, []),\n",
    "        query_timeout_multiplier=_QUERY_TIMEOUT_MULTIPLIER,\n",
    "        query_timeout_min_ms=_QUERY_TIMEOUT_MIN_MS,\n",
    "        query_timeout_max_ms=_QUERY_TIMEOUT_MAX_MS,\n",
    "        execute_query_fn=execution_method,\n",
    "        checkpoint_results_fn=checkpoint_results,\n",
    "        results_key=results_key,\n",
    "        limit=_LIMIT,\n",
    "        num_initial_default_executions=_NUM_INITIAL_DEFAULT_EXECUTIONS,\n",
    "        slowest_default_top_k=_SLOWEST_DEFAULT_TOP_K,\n",
    "        slowest_default_sample_size=_SLOWEST_DEFAULT_SAMPLE_SIZE,\n",
    "        plan_cover_num_params=_PLAN_COVER_NUM_PARAMS,\n",
    "        near_optimal_threshold=_NEAR_OPTIMAL_THRESHOLD,\n",
    "        num_params_threshold=_NUM_PARAMS_THRESHOLD,\n",
    "        query_timeout_minimum_speedup_multiplier=_QUERY_TIMEOUT_MINIMUM_SPEEDUP_MULTIPLIER,\n",
    "        previous_results=previous_results,\n",
    "        previous_metadata=previous_metadata,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426abbe5",
   "metadata": {},
   "source": [
    "# 2. Latency + Explain + Total Cost (Cái này có vẻ nặng, cân nhắc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_CARDINALITY_MULTIPLIERS = flags.DEFINE_list(\n",
    "    \"cardinality_multipliers\", None,\n",
    "    \"List of cardinality multipliers to apply when generating plans.\")\n",
    "flags.mark_flag_as_required(\"cardinality_multipliers\")\n",
    "\n",
    "_LIMIT = flags.DEFINE_integer(\n",
    "    \"limit\", 1,\n",
    "    \"Limit the number of parameters per query to gather cost estimates for.\")\n",
    "\n",
    "_KEYS_TO_REMOVE = flags.DEFINE_list(\n",
    "    \"keys_to_remove\", [],\n",
    "    (\"List of keys to filter from EXPLAIN plan JSON. Good candidates include \"\n",
    "     \"\\\"Parallel Aware\\\", \\\"Relation Name\\\", \\\"Parent Relationship\\\"\"))\n",
    "\n",
    "_QUERY = flags.DEFINE_string(\"query\", None, \"Specific query id to execute.\")\n",
    "\n",
    "_OUTPUT_DIR = flags.DEFINE_string(\"output_dir\", None,\n",
    "                                  \"Directory to store execution results.\")\n",
    "flags.mark_flag_as_required(\"output_dir\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CARDINALITY_MULTIPLIERS = [0.5, 1.0, 2.0]\n",
    "\n",
    "_LIMIT = None # Nên đặt giới hạn chỗ này cho đỡ chạy lâu\n",
    "\n",
    "_KEYS_TO_REMOVE = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_manager = query_utils.QueryManager(\n",
    "    query_utils.DatabaseConfiguration(\n",
    "        dbname=_DATABASE, user=_USER, password=_PASSWORD))\n",
    "query_utils.save_postgres_config_info(query_manager, _EXECUTION_OUTPUT_DIR)\n",
    "\n",
    "output_subdir = os.path.join(_EXECUTION_OUTPUT_DIR, \"execution_output\")\n",
    "os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "with open(_PLAN_HINTS_FILE) as f:\n",
    "    plan_hints = json.load(f)\n",
    "\n",
    "with open(_PARAMETER_VALUES_FILE) as f:\n",
    "    parameter_values = json.load(f)\n",
    "\n",
    "with open(_QUERY_TEMPLATES_FILE) as f:\n",
    "    templates = json.load(f)\n",
    "\n",
    "cardinality_multipliers = [\n",
    "    float(multiplier) for multiplier in _CARDINALITY_MULTIPLIERS\n",
    "]\n",
    "\n",
    "query_ids = [_QUERY] if _QUERY else plan_hints.keys()\n",
    "\n",
    "for query_id in query_ids:\n",
    "    results = pg_perturb_plan_cardinalities.multiplicatively_perturb_plan_cardinalities(\n",
    "        query_manager=query_manager,\n",
    "        query_id=query_id,\n",
    "        templates=templates,\n",
    "        parameter_values=parameter_values,\n",
    "        plan_hints=plan_hints,\n",
    "        cardinality_multipliers=cardinality_multipliers,\n",
    "        limit=_LIMIT,\n",
    "        keys_to_remove=_KEYS_TO_REMOVE)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(output_subdir, f\"{_DATABASE}_{query_id}.json\"),\n",
    "        \"w\") as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7e4ce",
   "metadata": {},
   "source": [
    "# IV. Vocabulary (Pending)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepler-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
