{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2c4b45",
   "metadata": {},
   "source": [
    "# Cấu hình chung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4db6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_DATABASE = flags.DEFINE_string(\"database\", None, \"Database name.\")\n",
    "flags.mark_flag_as_required(\"database\")\n",
    "_USER = flags.DEFINE_string(\"user\", None, \"Database username.\")\n",
    "_PASSWORD = flags.DEFINE_string(\"password\", None, \"Database password.\")\n",
    "_HOST = flags.DEFINE_string(\"host\", \"localhost\", \"Database host.\")\n",
    "_SEED = flags.DEFINE_float(\"seed\", 0, \"Database random number seed.\")\n",
    "\n",
    "_TEMPLATE_FILE = flags.DEFINE_string(\"template_file\", None,\n",
    "                                     \"Parameterized query template file.\")\n",
    "flags.mark_flag_as_required(\"template_file\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517a8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATABASE = 'kepler_stack'\n",
    "_USER = 'kepler_user'\n",
    "_PASSWORD = '12345'\n",
    "_HOST = 'localhost'\n",
    "_SEED = 0\n",
    "_TEMPLATE_FILE = 'inputs/stack_query_templates.json'\n",
    "\n",
    "_ROOT_OUTPUT_DIR = 'outputs'\n",
    "\n",
    "_QUERY = 'q1_0'\n",
    "\n",
    "_MAX_WORKERS = 1 # Giảm số worker để tránh quá tải CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abc55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(_ROOT_OUTPUT_DIR, exist_ok= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83daa264",
   "metadata": {},
   "source": [
    "# I. Gen các giá trị parameter cho query templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_PARAMETER_COUNT = flags.DEFINE_integer(\n",
    "    \"count\", 1000000, \"The max number of parameters to generate per query.\")\n",
    "\n",
    "_COUNTS_OUTPUT_FILE = flags.DEFINE_string(\n",
    "    \"counts_output_file\", None,\n",
    "    \"Output file to store the parameter counts per query.\")\n",
    "flags.mark_flag_as_required(\"counts_output_file\")\n",
    "_PARAMETERS_OUTPUT_DIR = flags.DEFINE_string(\n",
    "    \"parameters_output_dir\", None,\n",
    "    \"Directory to store parameter values per query.\")\n",
    "flags.mark_flag_as_required(\"parameters_output_dir\")\n",
    "\n",
    "_DRY_RUN = flags.DEFINE_bool(\n",
    "    \"dry_run\", False,\n",
    "    \"If true, verify that the parameter generation process works correctly \"\n",
    "    \"using a single, non-random parameter value. This involves a) verifying \"\n",
    "    \"that the parameter generation query can be composed from the template \"\n",
    "    \"query and b) ensuring that the template query executes successfully with \"\n",
    "    \"the generated parameter value.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27678427",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PARAMETER_COUNT = 1\n",
    "\n",
    "_COUNTS_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}/parameter_counts'\n",
    "_COUNTS_OUTPUT_FILE = f'{_COUNTS_OUTPUT_DIR}/parameter_counts.json'\n",
    "\n",
    "_PARAMETERS_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}/parameters/'\n",
    "\n",
    "_DRY_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f28d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(_COUNTS_OUTPUT_DIR, exist_ok= True)\n",
    "os.makedirs(_PARAMETERS_OUTPUT_DIR, exist_ok= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69402122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT site.site_name,tag.name\n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "AND site.site_name IS NOT NULL\n",
      "AND tag.name IS NOT NULL\n",
      "GROUP BY site.site_name,tag.name \n",
      "ORDER BY random()\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from concurrent import futures\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import parameter_generator\n",
    "import query_utils\n",
    "\n",
    "\n",
    "with open(_TEMPLATE_FILE) as f:\n",
    "  templates = json.load(f)\n",
    "\n",
    "work_list = []\n",
    "# Query templates that failed hint verification using the parameters from the\n",
    "# original Stack benchmark. That is, at least one provided hint was ignored by\n",
    "# the PG optimizer for at least one parameter binding.\n",
    "skip_list = [\"q3_0\", \"q3_1\", \"q3_2\"]\n",
    "for query_id, template in templates.items():\n",
    "  if query_id not in skip_list and query_id == _QUERY:\n",
    "    work_list.append(\n",
    "        parameter_generator.TemplateItem(\n",
    "            query_id=query_id, template=template))\n",
    "\n",
    "database_configuration = query_utils.DatabaseConfiguration(\n",
    "    dbname=_DATABASE,\n",
    "    user=_USER,\n",
    "    password=_PASSWORD,\n",
    "    host=_HOST,\n",
    "    seed=_SEED)\n",
    "generator = parameter_generator.ParameterGenerator(database_configuration)\n",
    "parameter_generation_function = functools.partial(\n",
    "    generator.generate_parameters, _PARAMETER_COUNT, dry_run=_DRY_RUN)\n",
    "\n",
    "output_counts = collections.defaultdict(lambda: {})\n",
    "# The high-latency work occurs remotely via the database executing queries to\n",
    "# generate parameters. The number of max workers is limited empirically to\n",
    "# avoid memory issues on the database side.\n",
    "with futures.ThreadPoolExecutor(max_workers=_MAX_WORKERS) as executor:\n",
    "  for result in executor.map(parameter_generation_function, work_list):\n",
    "    query_id = next(iter(result))\n",
    "    logging.info(\"Finished generating for %s\", query_id)\n",
    "    with open(\n",
    "        os.path.join(_PARAMETERS_OUTPUT_DIR,\n",
    "                      f\"{query_id}-{len(result[query_id]['params'])}.json\"),\n",
    "        \"w\") as f:\n",
    "      json.dump(result, f)\n",
    "\n",
    "    output_counts[query_id] = len(result[query_id][\"params\"])\n",
    "\n",
    "    if _DRY_RUN:\n",
    "      # Ensure that the template query executes successfully with the\n",
    "      # generated parameter value.\n",
    "      query_manager = query_utils.QueryManager(database_configuration)\n",
    "      start_ms = int(time.time() * 1e3)\n",
    "      query = result[query_id][\"query\"]\n",
    "      params = result[query_id][\"params\"][0]\n",
    "      results = query_manager.execute(query, params)\n",
    "      end_ms = int(time.time() * 1e3)\n",
    "\n",
    "      print(f\"Query {query_id} approximate latency: {end_ms-start_ms} ms\")\n",
    "      print(f\"Template: {query}\")\n",
    "      print(f\"Params: {params}\")\n",
    "\n",
    "      # The query should return at least one result.\n",
    "      assert results\n",
    "\n",
    "with open(_COUNTS_OUTPUT_FILE, \"w\") as f:\n",
    "  json.dump(output_counts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde840d",
   "metadata": {},
   "source": [
    "# II. Gen Plan Candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6b0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import json\n",
    "import os\n",
    "\n",
    "import main_utils\n",
    "import pg_generate_plan_candidates\n",
    "import pg_plan_hint_extractor\n",
    "import query_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689001da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationFunction(enum.Enum):\n",
    "  PG_CONFIGS = \"pg_configs\"\n",
    "  ROW_NUM_EVOLUTION = \"row_num_evolution\"\n",
    "  EXHAUSTIVE_CARDINALITY_PERTURBATIONS = \"exhaustive_cardinality_perturbations\"\n",
    "\n",
    "\n",
    "_GENERATION_FUNCTION_MAP = {\n",
    "    GenerationFunction.PG_CONFIGS:\n",
    "        pg_generate_plan_candidates.get_query_plans,\n",
    "    GenerationFunction.ROW_NUM_EVOLUTION:\n",
    "        pg_generate_plan_candidates.generate_by_row_num_evolution,\n",
    "    GenerationFunction.EXHAUSTIVE_CARDINALITY_PERTURBATIONS:\n",
    "        pg_generate_plan_candidates\n",
    "        .generate_by_exhaustive_cardinality_perturbations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf8f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _supports_distributed_execution(\n",
    "    generation_function: GenerationFunction) -> bool:\n",
    "  return generation_function != GenerationFunction.EXHAUSTIVE_CARDINALITY_PERTURBATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_QUERY_PARAMS_FILE = flags.DEFINE_string(\n",
    "    \"query_params_file\", None,\n",
    "    \"File containing parameterized queries with list of parameter values.\")\n",
    "flags.mark_flag_as_required(\"query_params_file\")\n",
    "_PARAMS_LIMIT = flags.DEFINE_integer(\n",
    "    \"params_limit\", None,\n",
    "    \"The number of parameter values to use when generating plans.\")\n",
    "_OUTPUT_DIR = flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"Directory in which to store query plan hints and configs.\")\n",
    "flags.mark_flag_as_required(\"output_dir\")\n",
    "\n",
    "_PLANS_OUTPUT_FILE = flags.DEFINE_string(\n",
    "    \"plans_output_file\", None,\n",
    "    \"File to store distinct plans per query. The file name is expected to end in .json\"\n",
    ")\n",
    "flags.mark_flag_as_required(\"plans_output_file\")\n",
    "_PLAN_INDEX_SUFFIX = flags.DEFINE_string(\n",
    "    \"plan_index_suffix\", \"_plan_index.json\",\n",
    "    \"Suffix of files to store plan indices in.\")\n",
    "_VERIFICATION_FAILURES_FILE = flags.DEFINE_string(\n",
    "    \"verification_failures_file\", \"verification_failures.json\",\n",
    "    \"Filename of file to save verification failures.\")\n",
    "_CHUNKSIZE = flags.DEFINE_integer(\n",
    "    \"chunksize\", 100, \"How many params to include in each subprocess chunk.\")\n",
    "_KEYS_TO_REMOVE = flags.DEFINE_list(\n",
    "    \"keys_to_remove\", [],\n",
    "    (\"List of keys to filter from EXPLAIN plan JSON. Good candidates include \"\n",
    "     \"\\\"Parallel Aware\\\", \\\"Relation Name\\\", \\\"Parent Relationship\\\"\"))\n",
    "\n",
    "_GENERATION_FUNCTION = flags.DEFINE_enum_class(\n",
    "    \"generation_function\", GenerationFunction.PG_CONFIGS.value,\n",
    "    GenerationFunction, \"Which plan generation function to use.\")\n",
    "_SOFT_TOTAL_PLANS_LIMIT = flags.DEFINE_integer(\n",
    "    \"soft_total_plans_limit\", None,\n",
    "    \"Soft limit on total number of plans to produce.\"\n",
    ")\n",
    "# Pg configs flags.\n",
    "_CONFIG_STR = flags.DEFINE_string(\n",
    "    \"configs\", \"\",\n",
    "    \"Comma-separated string of Postgres optimizer configuration parameters to toggle off.\"\n",
    ")\n",
    "# Row number evolution flags.\n",
    "_MAX_PLANS_PER_PARAM = flags.DEFINE_integer(\n",
    "    \"max_plans_per_param\", None,\n",
    "    \"Stop evolution after this number of plans is exceeded.\")\n",
    "_NUM_GENERATIONS = flags.DEFINE_integer(\n",
    "    \"num_generations\", 3, \"Number of generations of row number evolution.\")\n",
    "_NUM_MUTATIONS_PER_PLAN = flags.DEFINE_integer(\n",
    "    \"num_mutations_per_plan\", 25, \"Number of random mutations for each plan.\")\n",
    "_EXPONENT_BASE = flags.DEFINE_integer(\n",
    "    \"exponent_base\", 10, \"Base of exponential row number perturbations.\")\n",
    "_EXPONENT_RANGE = flags.DEFINE_integer(\n",
    "    \"exponent_range\", 3, \"One-sided range of exponent of perturbations.\")\n",
    "_MAX_PLANS_PER_GENERATION = flags.DEFINE_integer(\n",
    "    \"max_plans_per_generation\", 20,\n",
    "    \"Max number of plans to mutate per generation.\")\n",
    "_PERTURB_UNIT_ONLY = flags.DEFINE_bool(\n",
    "    \"perturb_unit_only\", True,\n",
    "    \"Whether to perturb only row counts exactly equal to one.\"\n",
    ")\n",
    "_MAX_PERTURBS_PER_JOIN = flags.DEFINE_integer(\n",
    "    \"max_perturbs_per_join\", 1,\n",
    "    \"Limit on how many times a specific join can be perturbed.\"\n",
    ")\n",
    "# Exhaustive cardinality perturbation flags.\n",
    "_CARDINALITY_MULTIPLIERS = flags.DEFINE_list(\n",
    "    \"cardinality_multipliers\", None,\n",
    "    \"List of cardinality multipliers to apply when generating plans.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc979618",
   "metadata": {},
   "outputs": [],
   "source": [
    "_QUERY_PARAMS_FILE = f'{_PARAMETERS_OUTPUT_DIR}/q1_0-1.json' # Thêm đường dẫn tới file json kết quả của phần I.\n",
    "_PARAMS_LIMIT = None\n",
    "_PLANS_OUTPUT_DIR = f'{_ROOT_OUTPUT_DIR}/plans'\n",
    "\n",
    "_PLANS_OUTPUT_FILE =  'pg_hints.json'\n",
    "\n",
    "_PLAN_INDEX_SUFFIX = '_plan_index.json'\n",
    "\n",
    "_VERIFICATION_FAILURES_FILE = 'verification_failures.json'\n",
    "\n",
    "_CHUNKSIZE = 100\n",
    "_KEYS_TO_REMOVE = []\n",
    "\n",
    "_GENERATION_FUNCTION = GenerationFunction.ROW_NUM_EVOLUTION\n",
    "\n",
    "_SOFT_TOTAL_PLANS_LIMIT = None\n",
    "# Pg configs flags.\n",
    "_CONFIG_STR = None\n",
    "\n",
    "# Row number evolution flags.\n",
    "_MAX_PLANS_PER_PARAM = 10\n",
    "\n",
    "_NUM_GENERATIONS = 3\n",
    "\n",
    "_NUM_MUTATIONS_PER_PLAN = 25\n",
    "\n",
    "_EXPONENT_BASE = 10\n",
    "_EXPONENT_RANGE = 3\n",
    "\n",
    "_MAX_PLANS_PER_GENERATION = 20\n",
    "\n",
    "_PERTURB_UNIT_ONLY = True\n",
    "\n",
    "_MAX_PERTURBS_PER_JOIN = 1\n",
    "\n",
    "# Exhaustive cardinality perturbation flags.\n",
    "_CARDINALITY_MULTIPLIERS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e356228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW seq_page_cost;\n",
      "SHOW random_page_cost;\n",
      "SHOW cpu_tuple_cost;\n",
      "SHOW cpu_index_tuple_cost;\n",
      "SHOW cpu_operator_cost;\n",
      "SHOW parallel_setup_cost;\n",
      "SHOW parallel_tuple_cost;\n",
      "SHOW min_parallel_table_scan_size;\n",
      "SHOW min_parallel_index_scan_size;\n",
      "SHOW effective_cache_size;\n",
      "SHOW jit_above_cost;\n",
      "SHOW jit_inline_above_cost;\n",
      "SHOW jit_optimize_above_cost;\n",
      "SHOW shared_buffers;\n",
      "SHOW huge_pages;\n",
      "SHOW temp_buffers;\n",
      "SHOW max_prepared_transactions;\n",
      "SHOW work_mem;\n",
      "SHOW hash_mem_multiplier;\n",
      "SHOW maintenance_work_mem;\n",
      "SHOW autovacuum_work_mem;\n",
      "SHOW max_stack_depth;\n",
      "SHOW shared_memory_type;\n",
      "SHOW dynamic_shared_memory_type;\n",
      "SHOW temp_file_limit;\n",
      "SHOW max_files_per_process;\n",
      "Start: {query_id}\n",
      "EXPLAIN (FORMAT JSON) select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(site tag #1000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #100000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10) Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #10) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) Rows(site tag #10) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #1000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #100) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #100000) Rows(site tag #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000) Rows(site tag #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000000) Rows(site tag #1000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #100) Rows(site tag #100000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(site tag #100) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #1000000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+ Rows(question site tag tag_question #10000) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "Checking 3 hints for query_id q1_0\n",
      "EXPLAIN (FORMAT JSON) /*+  SeqScan(site) SeqScan(tag) IndexOnlyScan(tag_question tag_question_site_id_tag_id_question_id_idx) IndexOnlyScan(question question_pkey) NestLoop(site tag) NestLoop(site tag tag_question) NestLoop(site tag tag_question question) Leading((((site tag) tag_question) question)) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+  SeqScan(tag) IndexOnlyScan(tag_question tag_question_site_id_tag_id_question_id_idx) IndexOnlyScan(question question_pkey) SeqScan(site) NestLoop(tag tag_question) NestLoop(tag tag_question question) HashJoin(tag tag_question question site) Leading((((tag tag_question) question) site)) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "EXPLAIN (FORMAT JSON) /*+  SeqScan(tag) SeqScan(site) IndexOnlyScan(tag_question tag_question_site_id_tag_id_question_id_idx) IndexOnlyScan(question question_pkey) MergeJoin(tag site) NestLoop(tag site tag_question) NestLoop(tag site tag_question question) Leading((((tag site) tag_question) question)) */ select count(*) \n",
      "from\n",
      " tag, site, question, tag_question\n",
      "where\n",
      "site.site_name='stackoverflow' and\n",
      "tag.name='connect-compose' and\n",
      "tag.site_id = site.site_id and\n",
      "question.site_id = site.site_id and\n",
      "tag_question.site_id = site.site_id and\n",
      "tag_question.question_id = question.id and\n",
      "tag_question.tag_id = tag.id\n",
      "Printing positive failure counts:\n",
      "Query q1_0 failure ratio: 0/3\n",
      "Printing hints counts by source:\n",
      "q1_0: number of suggestions for hint 0 from source default: 1\n",
      "q1_0: number of suggestions for hint 1 from source /*+ Rows(site tag #1000) */: 1\n",
      "q1_0: number of suggestions for hint 2 from source /*+ Rows(question site tag tag_question #1000) Rows(site tag #10) */: 1\n"
     ]
    }
   ],
   "source": [
    "configs = _CONFIG_STR.split(\",\") if _CONFIG_STR else []\n",
    "\n",
    "with open(_QUERY_PARAMS_FILE) as json_file:\n",
    "    info = json.load(json_file)\n",
    "\n",
    "hints_output_dir = os.path.join(_PLANS_OUTPUT_DIR, _DATABASE)\n",
    "os.makedirs(hints_output_dir, exist_ok=True)\n",
    "\n",
    "database_configuration = query_utils.DatabaseConfiguration(\n",
    "    dbname=_DATABASE,\n",
    "    user=_USER,\n",
    "    password=_PASSWORD,\n",
    "    host=_HOST)\n",
    "query_manager = query_utils.QueryManager(database_configuration)\n",
    "query_utils.save_postgres_config_info(query_manager, _PLANS_OUTPUT_DIR)\n",
    "\n",
    "hint_accumulator = main_utils.HintAccumulator()\n",
    "for query_id, query_metadata in info.items():\n",
    "    print(\"Start: {query_id}\")\n",
    "\n",
    "    output = {}\n",
    "    output[\"output\"] = {}\n",
    "\n",
    "    function_kwargs = {\n",
    "        \"database_configuration\": database_configuration,\n",
    "        \"query\": query_metadata[\"query\"],\n",
    "        \"keys_to_remove\": _KEYS_TO_REMOVE\n",
    "    }\n",
    "\n",
    "    # Augment kwargs depending on generation function.\n",
    "    if _GENERATION_FUNCTION == GenerationFunction.PG_CONFIGS:\n",
    "        function_kwargs[\"configs\"] = configs\n",
    "    elif _GENERATION_FUNCTION == GenerationFunction.ROW_NUM_EVOLUTION:\n",
    "        function_kwargs.update({\n",
    "            \"max_plans\": _MAX_PLANS_PER_PARAM,\n",
    "            \"num_generations\": _NUM_GENERATIONS,\n",
    "            \"num_mutations_per_plan\": _NUM_MUTATIONS_PER_PLAN,\n",
    "            \"exponent_base\": _EXPONENT_BASE,\n",
    "            \"exponent_range\": _EXPONENT_RANGE,\n",
    "            \"max_plans_per_generation\": _MAX_PLANS_PER_GENERATION,\n",
    "            \"perturb_unit_only\": _PERTURB_UNIT_ONLY,\n",
    "            \"max_perturbs_per_join\": _MAX_PERTURBS_PER_JOIN\n",
    "        })\n",
    "    elif _GENERATION_FUNCTION == GenerationFunction.EXHAUSTIVE_CARDINALITY_PERTURBATIONS:\n",
    "        cardinality_multipliers = [\n",
    "            float(multiplier) for multiplier in _CARDINALITY_MULTIPLIERS\n",
    "        ]\n",
    "\n",
    "        function_kwargs.update(\n",
    "            {\"cardinality_multipliers\": cardinality_multipliers})\n",
    "\n",
    "    if _PARAMS_LIMIT:\n",
    "        query_metadata[\"params\"] = query_metadata[\"params\"][:_PARAMS_LIMIT]\n",
    "\n",
    "    plan_hint_extractor = pg_plan_hint_extractor.PlanHintExtractor()\n",
    "    pg_generate_plan_candidates.execute_plan_generation(\n",
    "        _GENERATION_FUNCTION_MAP[_GENERATION_FUNCTION],\n",
    "        function_kwargs,\n",
    "        query_metadata[\"params\"],\n",
    "        plan_hint_extractor=plan_hint_extractor,\n",
    "        chunksize=_CHUNKSIZE,\n",
    "        distributed=_supports_distributed_execution(_GENERATION_FUNCTION),\n",
    "        soft_total_plans_limit=_SOFT_TOTAL_PLANS_LIMIT)\n",
    "    counts, plan_hints, params_plan_indices, debug_infos = (\n",
    "        plan_hint_extractor.get_consolidated_plan_hints())\n",
    "\n",
    "    hint_accumulator.query_id_to_counts[query_id] = counts\n",
    "    hint_accumulator.query_id_to_plan_hints[query_id] = plan_hints\n",
    "    hint_accumulator.query_id_to_params_plan_indices[\n",
    "        query_id] = params_plan_indices\n",
    "    hint_accumulator.query_id_to_debug_infos[query_id] = debug_infos\n",
    "\n",
    "    failure_counts = pg_plan_hint_extractor.verify_hints(\n",
    "        query_id=query_id,\n",
    "        query=query_metadata[\"query\"],\n",
    "        plan_hints=plan_hints,\n",
    "        params_plan_indices=params_plan_indices,\n",
    "        database_configuration=database_configuration)\n",
    "    hint_accumulator.combined_failure_counts.update(failure_counts)\n",
    "\n",
    "main_utils.print_failure_counts(hint_accumulator.combined_failure_counts)\n",
    "main_utils.print_hint_counts_by_source(hint_accumulator.query_id_to_counts)\n",
    "\n",
    "hint_accumulator.save(\n",
    "    output_dir=hints_output_dir,\n",
    "    plans_output_file=_PLANS_OUTPUT_FILE,\n",
    "    verification_failures_file=_VERIFICATION_FAILURES_FILE,\n",
    "    plan_index_suffix=_PLAN_INDEX_SUFFIX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepler-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
